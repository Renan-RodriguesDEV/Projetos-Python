{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "data_base_path = 'C:\\\\Users\\\\User\\\\Downloads\\\\Imagens'\n",
    "test_path= os.path.join(data_base_path,'test').replace(os.path.sep,\"\\\\\")\n",
    "train_path=os.path.join(data_base_path,'train').replace(os.path.sep,\"\\\\\")\n",
    "validation_path=os.path.join(data_base_path,'validation').replace(os.path.sep,\"\\\\\")\n",
    "print(test_path)\n",
    "print(train_path)\n",
    "print(validation_path)\n",
    "\n",
    "heigth,width=180,180 #config img\n",
    "epochs = 20 # qnts epocas ele ira iterar para o train\n",
    "batch_size = 32 # tamanho dos lotes\n",
    "learnig_rate=0.0001 # taxa de aprendizagem para o optimizer\n",
    "\n",
    "classes = ['ausente','escassa','uniforme','mediana']\n",
    "count_class = len(classes)\n",
    "\n",
    "# data set treino\n",
    "dataset_treino = keras.utils.image_dataset_from_directory(\n",
    "    train_path,\n",
    "    image_size=(width,heigth),\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# data set validacao\n",
    "dataset_validacao = keras.utils.image_dataset_from_directory(\n",
    "    validation_path,\n",
    "    image_size=(width,heigth),\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# Dividindo o conjunto de validação em validação e teste\n",
    "val_batches = tf.data.experimental.cardinality(dataset_validacao)\n",
    "dataset_teste = dataset_validacao.take(val_batches // 2)\n",
    "dataset_validacao = dataset_validacao.skip(val_batches // 2)\n",
    "\n",
    "# Contando o número de amostras em cada conjunto\n",
    "num_val_samples = sum(len(batch) for batch in dataset_validacao)\n",
    "num_test_samples = sum(len(batch) for batch in dataset_teste)\n",
    "\n",
    "print(f\"Número de amostras na validação: {num_val_samples}\")\n",
    "print(f\"Número de amostras no teste: {num_test_samples}\")\n",
    "\n",
    "from keras.layers import RandomFlip,RandomRotation,RandomZoom\n",
    "\n",
    "# data argumentation pra evitar overfit\n",
    "data_argumentation = Sequential([\n",
    "    RandomFlip('horizontal_and_vertical'),\n",
    "    RandomRotation(0.2),\n",
    "    RandomZoom((0.2,0.3))\n",
    "])\n",
    "\n",
    "# rescala\n",
    "escala = layers.Rescaling(scale=1./255)\n",
    "\n",
    "# criacao do modelo\n",
    "modelo = Sequential([\n",
    "    data_argumentation,\n",
    "    escala,\n",
    "    layers.Conv2D(32,(3,3),padding='same',activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Conv2D(64,(3,3),padding='same',activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Conv2D(128,(3,3),padding='same',activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Conv2D(256,(3,3),padding='same',activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256,activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(num_classes,activation='softmax')\n",
    "])\n",
    "\n",
    "# copilacao do modelo \n",
    "modelo.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    "  )\n",
    "\n",
    "# resumo do modelo \n",
    "modelo.summary()\n",
    "\n",
    "# treinamento do modelo\n",
    "\n",
    "history = modelo.fit(\n",
    "    dataset_treino, validation_data=dataset_validacao, epochs=epochs, \n",
    ")               \n",
    "\n",
    "# Avaliação do modelo no conjunto de teste\n",
    "test_loss, test_acc = modelo.evaluate(dataset_teste, verbose=2)\n",
    "print(\"\\nTest accuracy:\", test_acc)\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Função para carregar e preprocessar a imagem\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(height, width))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Adiciona uma dimensão extra para o batch\n",
    "    img_array = img_array / 255.0  # Normaliza a imagem como feito no treinamento\n",
    "    return img_array\n",
    "\n",
    "# Caminho para a imagem que você quer testar\n",
    "img_path = 'C:\\\\Users\\\\User\\\\Downloads\\\\Imagens\\\\test_image.jpg'\n",
    "\n",
    "# Preprocessa a imagem\n",
    "img_array = preprocess_image(img_path)\n",
    "\n",
    "# Faz a predição\n",
    "predictions = modelo.predict(img_array)\n",
    "\n",
    "# Obtém a classe prevista\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "predicted_class_label = classes[predicted_class[0]]\n",
    "\n",
    "print(f\"Predição: {predicted_class_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
